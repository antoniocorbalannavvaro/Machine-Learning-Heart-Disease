{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto de Machine Learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Detección y prevención de enfermedades cardiovasculares*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que he hecho ha sido realizar una busquedad de datos. \n",
    "En este proyecto utilizaremos un dataset sacado de kaggle. [Link del dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente he limpiado los datos del dataset. Principalmente he realizdo conversiones de variables tipo texto a variables numéricas, para poder hacer un análisis posterioremente y poder entrenar nuestros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el proceso de limpieza de datos he visto oportuno exportar dos datasets:\n",
    "1. Un datset con las variables de tipo texto convertidas a tipo numérico. (Para el análisis)\n",
    "2. El mismo dataset pero con los datos estandarizados (-1,1) (Para los modelos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Análisis exploratorio y documentación sobre el tema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro segundo paso ha sido realizar un análisis exploratorio de datos para ver qué variables son las más importantes y como están relacionadas entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto conseguimos tener una idea más clara del tipo de datos que estamos tratando.\n",
    "Para realizar este análisis he tenido que documentarme en paginas especializadas en tema de las enfermedades cardiovasculares, ya que hay bastantes tecnicismos y vocabulario que me eran ajenos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Creación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear el modelo definitivo he tenido que probar todos los modelos posibles para poder descartar los modelos con menor precisión. Como se trata de un problema de clasificación, todos los algoritmos que usemos de este tipo nos darán buenos resultados. Por eso probaremos todos los posibles para elegir el más acertado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se trata de un problema de clasificación hemos desechado los modelos de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de descartar los modelos de regresión me puse a crear modelos de clasificación, que son los más acertados y efecientes en este tipo de problemas, dandome resultados que superaban el 80% de acierto en la mayoría de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los modelos que mejor rendimiento me dieron fueron los siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. K Neighbors Classifier: 86,59 %\n",
    "\n",
    "2. Regresión Logística: 84,74 %\n",
    "\n",
    "3. Decision Tree Classifier: 84,42 %\n",
    "\n",
    "4. Decision Tree Classifier (Bagging): 91,72 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Entrenamiento y exportación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro modelo final es un árbol de decisiones al que le hemos aplicado un algoritmo de \"Bagging\" para poder crear, en nuestro caso, 50 árboles diferentes y sacar el promedio.\n",
    "Sin aplicar el algoritmo de bagging obtenemos un 84,42% de acierto, mientras que utilizándolo obtenemos un 91,72 %.\n",
    "\n",
    "De esta manera nuestro modelo generaliza mejor ante nuevos datos, evitando asi problemas de overfitting, ya que un solo árbol puede tender a aprenderse los datos de memoria sin crear patrones generales para diferentes registros."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afc7916f6869e42630546bdae4a8ce3ae8c6a9be38a4ebefb81f2a8bf402afe1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
